# Optimization
Optimization involves finding the parameter $\theta$ of a neural network that significantly reduces a cost function $J(\theta)$, which includes a performance measure evaluated on the entire training set as well as regularization terms. 

In ML, we care about some performance measure $P$, which is defined with respect to a training set and may also be intractable. We optimize $P$ indirectly by reducing a cost function $J(\theta)$ hoping that will improve $P$, which is different from pure optimization where minimizing $J$ is a goal in and of itself. Expectation across data-generating distribution is: $J^*(\theta) = \mathbb{E}_{(x, y)\sim p_{data}} L(f(x;\theta), y)$, where $L$ is the per example loss function. The goal of ML is to reduce the expected generalization error. This quantity is known as the risk. The training process based on minimizing the average training error is known as *empirical risk minimization*. Rather than optimizing the risk directly, we optimize the empirical risk and hope that the risk decreases significantly as well. Empirical risk minimization is prone to overfitting. 

Surrogate loss function acts as a proxy to the loss function we care about, such as the negative log-likelihood of the correct class is used as a surrogate for 0-1 loss function, which has no useful derivatives (derivatives are either zero or undefined everywhere). The negative log-likelihood allows the model to estimate the conditional probability of the classes given the input. If the model can do well, then it can pick the classes that yield the least classification error. 

The main difference between optimization in general and optimization as we use for training algorithms is that they do not usually halt at a local minimum. Instead, an ML algorithm usually minimizes a surrogate loss function but halts when a convergence criteria based on early stopping is satisfied. 

Another difference is the objective function usually decomposes as a sum over the training examples. Computing expectations exactly is very difficult because it requires evaluating the model over every example in the dataset. In practice, these expectations are computed by randomly sampling a small number of examples from the dataset and taking the average over those samples. The standard error of mean is $\sigma/\sqrt{n}$, where $\sigma$ is the true standard deviation. Consider two sample sizes of 100 and 10,000, the 10,000 sample size requires 100 times more computation than the sample size of 100, but it only reduces the standard error by a factor of 10. 

A statistical estimation of the gradient is also beneficial as, in practice, there may be redundant examples in the training set or examples that make very similar contributions to the gradient. "Batch gradient" means an algorithm that uses the *full training set*. The size of examples used in stochastic methods depend on:
- Large batch sizes are more accurate but with less than linear returns
- Multicore architectures are less effectively utilized by small batch sizes
- The amount of memory required scales with the batch size and limits the batch sizes
- Smaller batch sizes add some regularization effect by adding noise to the learning process. This requires a smaller learning rater and hence is slow.

Some methods are sensitive to sampling effects. _Methods that compute the gradient $g$ are usually relatively robust and can handle small batches like 100. Second-order methods that compute the Hessian matrix $H^{-1}g$ require larger batch sizes like 10,000 to minimize fluctuations in the computation of $H^{-1}g$_. 

Minibatches are computed based on random sampling and require the samples to be independent. Two subsequent gradient estimates, and therefore, two subsequent mini-batches must also be independent of each other. Many examples in the training set are arranged such that they are highly correlated, e.g., a dataset of blood samples taken at different times for different patients arranged by the patient order. Shuffling these examples usually reduces this error. _Failing to shuffle the examples in any way can seriously reduce the effectiveness of any algorithm_.

SGD minimizes the generalization error. Most implementation of minibatch SGD shuffles the dataset once and then pass through it multiple times. On the first pass, each minibatch computes an unbiased estimate of the true generalization error. The estimates become biased in the second and subsequent passes as it is formed by resampling rather than new fair samples. Minibatch computes: $\hat{g} = \frac{1}{m} \nabla_\theta \sum_i L(f(x^{(i)};\theta), y^{(i)})$. Updating $\theta$ in the direction of $\hat{g}$ performs SGD on the generalization error. This only applies when the examples are not reused. In practice with the large growing dataset, each training example is used only once or even making incomplete passes through the training set. When using an extremely large training set, overfitting is no longer an issue. 

## Challenges in neural network optimization
Optimization in the neural network involves designing objective functions that are convex. Even convex functions have challenges when training neural nets. 

### Illconditioning
All condition can manifest by causing SGD to get "stuck" in the sense that even very small steps increase the cost function. The second-order Taylor series of the cost function predicts a gradient descent step of $\epsilon g$ will add $\frac{1}{2}\epsilon^2 g^T Hg - \epsilon g^T g$ to the cost function. Illconditioning of the gradient causes a problem when $\frac{1}{2}\epsilon^2 g^T Hg$ term is larger than $\epsilon g^T g$. In many cases, the gradient norm does not shrink significantly throughout the learning, but $\epsilon g^T g$ term grows by more than an order of magnitude. The learning becomes very slow despite steep gradients because the learning rater must shrink to compensate for the drastic changes in the curvature. 

### Local minima
Some convex functions have a flat region at the bottom rather than a single global minimum point. Any point on the flat region is an acceptable solution. DL models have a large number of local minima and can have higher costs than the global minimum. In practice, however, with sufficiently large neural networks, most local minima have a low-cost function value. It is important to find the true local minima rather than a point in parameter space that has a low but not minimal cost. A test that can rule out local minima is to plot the gradient with time. If the norm of gradient does not shrink to insignificant size, the problem is neither local minima nor any kind of critical points. 

### Plateaus, saddle points, and other flat regions
We can think of saddle points as local minima along one cross-section and a local maximum along another cross-section of the cost function. In low dimensions, the local minima are common. In high dimensions, local minima are rare, and saddle points are more common. 

Gradient descent is designed to move "downhill" and is not explicitly designed to seek a critical point. However, Newton's method is designed to solve for a point where the gradient is zero without modification. It can jump to a saddle point. 

### Cliffs and exploding gradients:
Neural networks with many layers have extremely steep gradients, resembling cliffs. Gradients may move fast and usually jump off the cliff structure. Cliffs are more common in Recurrent Neural Networks (RNNS) as it involves multiplication of many factors. 

### Long term dependencies
Repeated application of the same parameters, like in RNNs, gives rise to especially pronounced difficulties. Suppose we multiply by a matrix $W$ at each step; after $t$ steps, it is equivalent to $W^t$. If $W$ has an eigendecomposition of $= V diag(\lambda) V^{-1}$, then $W^t = (V diag(\lambda) V^{-1})^t = V diag(\lambda)^t V^{-1}$. Any eigenvalues $\lambda_i$ which are not near an absolute value of one, will either explode if they are greater than one or vanish if they are less than one. Vanishing gradients make it difficult to know which direction the parameter should move to improve the cost function. Deep feedforward networks avoid multiplying by $W$ so avoids the vanishing and exploding gradient problem, while RNNs do not. 

### Inexact gradients
Nearly all DL algorithms depend on the gradients' sampling-based estimates, such as using mini-batch in computing gradients of the training example. When the objective function is intractable, its gradient is also intractable. Neural networks are designed to account for imperfections in gradient estimates. One can avoid this issue by choosing a surrogate loss function than the true loss function. 

Much of the research is focused on difficulties of optimization when training arrives at a global minimum, local minimum, or saddle points. Neural networks do not often arrive at a region of smaller gradients. Indeed such critical points do not necessarily exist, e.g., loss function $- \log p(y|x; \theta)$ can lack a global minimum point and instead asymptotically approach some value as the model becomes more confident. In low dimensions, starting on the wrong side of the cost function hill, the algorithm may get stuck on this wrong side. The algorithm could potentially circumnavigate the hill in high dimensions, but it results in longer training trajectories and excessive training. 

Gradient descent and other learning algorithms usually make small moves. The gradient of the objective function is computed approximately by sampling with bias or variance in the estimate of the correct direction. The objective function may suffer from poor conditioning, causing the region where the gradient provides a good model of the objective function to be very small. In these cases, local descent with a step size of $\epsilon$ may define a reasonably short path to the solution, but we can only compute the local descent direction with step size $\delta << \epsilon$. In these cases, although local descent may define a path to the solution, the path contains many steps and incurs high computation cost. Local descent could lead to moving in the wrong direction or a larger number of steps required to reach the solution, which of these is a more relevant problem that makes optimization of neural network difficult is still an open problem. Hence, choosing good initial points for the optimization algorithm is essential.