# Building an ML algorithm
Every ML algorithm has four components: 
- Specification of a dataset
- Cost function
- Optimization process (gradient descent)
- Model

These four components can be mixed and matched to generate a wide range of algorithms. A commonly used _cost function_ is the negative log-likelihood. Minimizing the cost function gives the maximum likelihood estimate. Cost function also has additional terms like regularization (weight decay in linear regression).

## The curse of dimensionality

ML problems become exceedingly difficult when the number of dimensions in the data is high, i.e., the number of possible distinct configurations of a set of variables increases exponentially as the number of variables increases. Consider an example dataset with ten variables in 1D, and we can define a target function for each and generalize. As we move towards 2D, we need to keep track of $10\times 10=100$ regions now, and in 3D, this increases to $10^3 = 1000$ regions. For dimension $d$ and values $v$ we need $O(v^d)$ examples and regions. In high dimensions, the number of configurations is huge, much larger than our number of examples. In high dimensional conditions with the lack of data, a typical grid with no training example is approximated by the nearest training point's output. 

### Local constancy and smoothness regularization
Prior beliefs guide ML about the kind of function they should learn. We incorporate the priors _explicitly_ as a probability distribution over parameters of the model. Prior beliefs directly influence the function and indirectly the parameters. Prior beliefs can be incorporated _implicitly_ by choice of the algorithm biased towards a family of functions. 

A **smoothness or local constancy prior** is an implicit prior that states the function we learn should not change very much within a small region. Many simpler algorithms depend on this before generalization, as a result, fail to scale. 

Learning process involves learning a function $f^*$ that satisfies the condition $f^*(x) \approx f^*(x + \varepsilon)$ for most configurations $x$ and a small change $\varepsilon$. There are many different ways to _implicitly_ or _explicitly_ express a prior belief that this learned function should be smooth or locally constant. If we have several good answers around $x$, we could combine them (average or interpolate) to produce an answer that agrees with so many of them as possible. In k-nearest neighbors, the predictors are literally exact over a region containing all $x$ points. When $k=1$, the number of regions cannot be more than the training set. 

**Local kernel**, where $k(u, v)$ is large when $u=v$ and decreases as $u$ and $v$ grows further apart. A local kernel can be thought of as a similarity function that performs template matching by measuring how close a test example $x$ resemble each training $x^{(i)}$.

In general, to distinguish $O(k)$ regions in input space, we need $O(k)$ examples (e.g., decision trees and the discretization of input space). Typically there are $O(k)$ parameters, with $O(1)$ parameters associated with each of the $O(k)$ regions. Each training example only informs the learner about how to generalize in some neighborhood immediately surrounding that example. Smoothness function and the non-parametric learning works well as long as there are enough examples to observe high points on peaks and low points on valleys of the underlying function. If the function behaves differently in different dimensions and additionally behaves differently in various regions, it becomes extremely complicated to describe the behavior. 

Very large regions $O(2^k)$ can be defined with $O(k)$ examples so long as we introduce some dependencies between the regions through additional assumptions about the underlying data-generation distribution, such as a test-specific assumption like periodicity in the dataset. Usually, we do not include such task-specific assumptions in neural networks so that they can generalize to a much wider variety of structures. In DL, we assume that the data was generated by the composition of factors or features potentially in multiple levels in the hierarchy. 

## Manifold learning
The manifold is a connected region or set of points associated with a neighborhood around each point. From any given point, manifold appears to be a Euclidean space. For example, we observe the surface of the world as a two-dimensional plane, although it is a 3D spherical manifold. 

In ML, the manifold is used to refer to a connected set of points that can be approximated well by considering a small number of degrees of freedoms or dimensions embedded in higher dimensions (e.g., a 1D string in a 2D space). In ML, we allow the dimensionality of the manifold to vary from one point to another (e.g., a figure 8-shaped manifold in 1D in most spaces and 2D at the intersection of the loops). 

To learn functions with interesting variations across all of $\mathbb{R}^n$, manifold learning assumes most of $\mathbb{R}^n$ consists of invalid inputs and interesting inputs occur only along with a collection of manifold containing a small subset of points with interesting variations occurring in the output of the learned function only along with directions that lie on the manifold or when we move from one manifold to another. The assumption is that probability mass density is highly concentrated (e.g., the probability of picking random characters to form a meaning English sentence is almost zero). Many manifolds are likely to be involved in an application (human and cat faces are two different manifolds). 